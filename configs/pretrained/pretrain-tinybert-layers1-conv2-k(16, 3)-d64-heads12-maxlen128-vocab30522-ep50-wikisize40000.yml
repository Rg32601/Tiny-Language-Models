model_name: pretrain-tinybert-layers1-conv2-k(16, 3)-d64-heads12-maxlen128-vocab30522-ep50-wikisize40000
architecture: TinyBERT + Multi-Kernel Convolution
num_transformer_layers: 1
num_conv_layers: 2
conv_kernel_sizes: (16,3)
conv_channels: 64
num_attention_heads: 12
max_sequence_length: 128
vocab_size: 30522
pretrain_epochs: 50
pretrain_data: Wikipedia
pretrain_data_size: 40000
files:
  - config.json
  - model.safetensors
description: |
  TinyBERT-style language model with a two-layer convolutional front-end.
  - 1 transformer layer
  - 2 convolutional layers with kernel sizes 16 and 3, channels 64
  - 12 attention heads
  - Max sequence length 128, vocab size 30,522
  - Trained for 50 epochs on a 40,000-sample subset of Wikipedia
usage: |
  from configs.utils import download_and_load_pre_trained_model
  model = download_and_load_pre_trained_model("pretrain-tinybert-layers1-conv2-k(16, 3)-d64-heads12-maxlen128-vocab30522-ep50-wikisize40000")
notes: |
  - The two convolutional layers use kernel sizes (16,3).
  - Useful for experimenting with deeper and variable kernel convolutional front-ends.
  - For more usage details, see the main README.
